# -*- coding: utf-8 -*-
"""assignment 4 deep_fashion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HjR3vur-b4SeTj8Y3662phs5wCz6ftIr

# Assignment 4 

Using a pre-trained model, build a deep network that
predicts the category and attributes of an item simultaneously.

image from : https://drive.google.com/file/d/1alC3j-4yaHfZe4bYkr6M7Snxeru4GzYN/view

• Category (multi class classification): 10

• Attribute (multi label classification): 15

## Load the package and data
"""

!python --version
!pip freeze | grep torch
from google.colab import drive
drive.mount('/content/drive')
!unzip -qq ./drive/My\ Drive/deep_fashion.zip

# labels = torch.LongTensor([1, 2, 4])
# y_onehot = nn.functional.one_hot(labels, num_classes=15)
# y_onehot = y_onehot.sum(dim=0).float()
# print(y_onehot)

"""## Data preprocessing"""

from torch import Tensor

import torch.nn as nn 
import csv
import os
import numpy as np
from PIL import Image
import torch
from torchvision import transforms
import tensorflow as tf


transform_train = transforms.Compose([
 transforms.Resize((256,256)),
 transforms.RandomCrop((224,224)),
 transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),
 transforms.ToTensor(),
 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
])

transform_test = transforms.Compose([
transforms.Resize((256,256)),
 transforms.RandomCrop((224,224)),
 transforms.ToTensor(),
 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),                                
])



class FashionData(torch.utils.data.Dataset):
    def __init__(self, csv_file, mode='train', transform=None):
        
        self.mode = mode # 'train', 'val' or 'test'
        self.transform = transform

        self.data_list = []
        self.class_label = []
        self.attri_label = []
        with open(csv_file, newline='') as csvfile:
          reader = csv.reader(csvfile)
          for id, row in enumerate(reader):
              if id != 0:
                self.data_list.append(row[0]) # file_path
                if mode != 'test':                 
                  self.class_label.append(int(row[1]))  # class
                  b = torch.tensor(list(map(int, row[2].split(" "))),dtype=int) # attribution
                  y_onehot = nn.functional.one_hot(b, num_classes=15).sum(dim=0).int()                  
                  y_onehot = y_onehot.numpy().tolist()
                  self.attri_label.append(y_onehot)
                  # print(type(y_onehot))


    def __getitem__(self, index):

        data = Image.open(self.data_list[index])
        if self.transform is not None:
            data = self.transform(data)
        if self.mode == 'test':
            return data

        class_label = torch.tensor(int(self.class_label[index]))   
        attri_label = torch.tensor(self.attri_label[index])

        return data, class_label, attri_label

    def __len__(self):
        return len(self.data_list)

    def file(self,idx):
      return self.data_list[idx]    


# instantiate deepfashion
dataset_train = FashionData('./deep_fashion/train.csv', mode='train',transform=transform_train)
dataset_val = FashionData('./deep_fashion/val.csv', mode='val',transform=transform_test)
dataset_test = FashionData('./deep_fashion/test.csv', mode='test',transform=transform_test)

print("The first image's shape in dataset_train :", dataset_train.__getitem__(0)[0].size())
print("There are", dataset_train.__len__(), "images in dataset_train.")

from torch.utils.data import DataLoader

train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True,num_workers=10 )
val_loader = DataLoader(dataset_val, batch_size=128, shuffle=False,num_workers=10 )
test_loader = DataLoader(dataset_test, batch_size=128, shuffle=False,num_workers=10 )

"""## Resnet50 Model"""

import torch.nn as nn 
import torch.nn.functional as F
import torchvision

class CNN_Model(nn.Module): 

  def __init__(self): 
    # super().__init__()
    super(CNN_Model, self).__init__()
    self.resnet_model = torchvision.models.resnet34(pretrained=True) 
    self.resnet_model.train()
    for param in self.resnet_model.parameters():
      param.requires_grad = False
    self.resnet_model.avgpool = nn.AdaptiveAvgPool2d((1, 1))

    self.flat = None

    # for classification 0-9 categories
    self.fc_model1 = nn.Sequential(
        nn.Linear(1000,400),
        nn.BatchNorm1d(400),
        nn.LeakyReLU(),
				nn.Linear(400,10),
				nn.Softmax(dim=1),
      )
    
    # for attribute 15 attributions 
    self.fc_model2 = nn.Sequential( 
        nn.ReLU(),
        nn.Linear(1000,500),
        nn.ReLU(),
        nn.Dropout(),
        nn.Linear(500,100),
        nn.BatchNorm1d(100),
        nn.ReLU(),
        nn.Dropout(),
        nn.Linear(100,15),      
        nn.Sigmoid() 	
      )
    

  def forward(self, x):
    if not isinstance(x, torch.Tensor):
      x=torch.Tensor(x)	 
    out = self.resnet_model(x)
    out1 = self.fc_model1(out)
    out2 = self.fc_model2(out)
 
    return out1, out2

import torch

model = CNN_Model()
device = 'cuda'
model = model.to(device)

from torchsummary import summary
summary(model,(3, 256, 256))

"""### Define loss and optimizer"""

import torch.nn as nn
import torch.optim as optim

criterion1,criterion2 = nn.CrossEntropyLoss(), nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
criterion1, criterion2 = criterion1.cuda(), criterion2.cuda()

"""## Training the model 
for classification and attribution
"""

from sklearn.metrics import f1_score

def train(input_data, model, optimizer):
    model.train()
    loss_list1 = []
    loss_list2 = []
    total_count1 = 0 
    total_count2 = 0
    acc_count1 = 0
    acc_count2 = 0
    acc1 = 0
    loss1 = 0
    for i, data in enumerate(input_data, 0):
        images, class_labels, attr_labels = data[0].cuda(), data[1].cuda(), data[2].cuda()
        
        optimizer.zero_grad()
        out1, out2 = model(images)
        
        # for classification
        loss1 = criterion1(out1, class_labels)

        # for attribution
        loss2 = criterion2(out2, attr_labels.type(torch.float))

        loss = loss1 + loss2
        loss.backward()
        optimizer.step()
        
        # classification 
        _, predicted1 = torch.max(out1.data,1)
        total_count1 += class_labels.size(0)            
        acc_count1 += (predicted1 == class_labels).sum().item()
        loss_list1.append(loss1.item())

        # attribution   
        attr_labels = attr_labels.to(torch.int32)
        out2 = torch.where(out2 > 0.5, 1, 0)
        out2 = out2.to(torch.int32)
               
        acc_count2 +=  f1_score(attr_labels.cpu().numpy(), out2.cpu().numpy(), average='samples')
        loss_list2.append(loss2.item())

    # Compute classification epoch accuracy and loss
    acc1 = acc_count1 / total_count1
    loss1 = sum(loss_list1) / len(loss_list1)

    # Compute attribution epoch accuracy and loss
    acc2 = acc_count2 
    loss2 = sum(loss_list2) / len(loss_list2)

    return acc1, acc2 , loss1, loss2

"""### validation function """

from sklearn.metrics import f1_score

def val(input_data, model):
    model.eval()
    loss_list2 = []
    loss_list1 = []
    total_count1 = 0
    total_count2 = 0
    acc_count1 = 0
    acc_count2 = 0
    acca = 0
    acc1 = 0
    loss1 = 0
    with torch.no_grad():
        for data in input_data:
            images, class_labels, attr_labels = data[0].cuda(), data[1].cuda(), data[2].cuda()
            
            out1, out2 = model(images)

            loss1 = criterion1(out1, class_labels)
            loss2 = criterion2(out2, attr_labels.type(torch.float))
            loss = loss1 + loss2

            # classification 
            _, predicted1 = torch.max(out1.data,1)
            total_count1 += class_labels.size(0)            
            acc_count1 += float(torch.sum(predicted1 == class_labels))
            loss_list1.append(loss1.item())

            # attribution 
            attr_labels = attr_labels.to(torch.int32)
            out2 = torch.where(out2 > 0.5, 1, 0)
            out2 = out2.to(torch.int32)
            loss_list2.append(loss2.item())              
            acc_count2 +=  f1_score(attr_labels.cpu(), out2.cpu(), average='samples')

                      
            # print("acc_count2 = ",acc_count2)
            # print("acc2 = acc_count2 / total_count2 = ",acc_count2 / total_count2)
    

    # Compute classification epoch accuracy and loss
    acc1 = acc_count1 / total_count1
    loss1 = sum(loss_list1) / len(loss_list1)

    # Compute classification epoch accuracy and loss
    acc2 = acc_count2 
    loss2 = sum(loss_list2) / len(loss_list2)

    return acc1, acc2 , loss1, loss2

"""## Training in a loop"""

max_epochs = 20
log_interval = 2 # print acc and loss in per log_interval time

train_acc_list1 = []
train_loss_list1 = []
train_acc_list2 = []
train_loss_list2 = []
val_acc_list1 = []
val_loss_list1 = []
val_acc_list2 = []
val_loss_list2 = []


for epoch in range(1, max_epochs + 1):
    tacc1, tloss1, tacc2, tloss2 = train(train_loader, model, optimizer)
    vacc1, vloss1, vacc2, vloss2  = val(val_loader, model)

    # classification 
    train_acc_list1.append(tacc1)
    train_loss_list1.append(tloss1)
    val_acc_list1.append(vacc1)
    val_loss_list1.append(vloss1)

    # attribution 
    train_acc_list2.append(tacc2)
    train_loss_list2.append(tloss2)
    val_acc_list2.append(vacc2)
    val_loss_list2.append(vloss2)


    if epoch % log_interval == 0:
        # print("_________________classicfication__________________")
        print('=' * 20, 'Epoch', epoch, '=' * 20)
        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(tacc1, tloss1))
        print('Val Acc: {:.6f}   Val Loss: {:.6f}'.format(vacc1, vloss1))


    if epoch % log_interval == 0:
        print("___________________attribution____________________")
        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(tacc2, tloss2))
        print('Val Acc: {:.6f}   Val Loss: {:.6f}'.format(vacc2, vloss2))
        print('Val Acc: {:.6f}   Val Loss: {:.6f}'.format(vacc2, vloss2))

"""## Visualize accuracy and loss"""

import matplotlib.pyplot as plt

# classicfication 
plt.figure(figsize=(12, 4))
plt.plot(range(len(train_loss_list1)), train_loss_list1)
plt.plot(range(len(val_loss_list1)), val_loss_list1, c='r')
plt.legend(['ctrain', 'cval'])
plt.title('Loss')
plt.show()
plt.figure(figsize=(12, 4))
plt.plot(range(len(train_acc_list1)), train_acc_list1)
plt.plot(range(len(val_acc_list1)), val_acc_list1, c='r')
plt.legend(['ctrain', 'cval'])
plt.title('Acc')
plt.show()


# for attribution 
plt.figure(figsize=(12, 4))
plt.plot(range(len(train_loss_list2)), train_loss_list2)
plt.plot(range(len(val_loss_list2)), val_loss_list2, c='r')
plt.legend(['ttrain', 'tval'])
plt.title('Loss')
plt.show()
plt.figure(figsize=(12, 4))
plt.plot(range(len(train_acc_list2)), train_acc_list1)
plt.plot(range(len(val_acc_list2)), val_acc_list2, c='r')
plt.legend(['ttrain', 'tval'])
plt.title('Acc')
plt.show()

"""## Predict Result"""

def predict(input_data, model):
    model.eval()
    listout_1 = []
    listout_2 = []
    with torch.no_grad():
        for data in input_data:
            images = data.cuda()
            outputs1, outputs2 = model(images)
            classicfication
            _, predicted1 = torch.max(outputs1.data, 1)
            attribution 
            predicted2 = torch.where(outputs2 > 0.5, 1, 0)

            listout_1.extend(predicted1.to('cpu').numpy().tolist())
            listout_2.extend(predicted2.to('cpu').numpy().tolist())

    return listout_1 , listout_2

idx,idex= 0
outputc,outputa = predict(test_loader, model)

with open('result_c.csv', 'w', newline='') as csvFile:
    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'category_label'])
    writer.writeheader()
    for result in outputc:
        file_path = dataset_test.file(idx)
        idx+=1
        writer.writerow({'file_path':file_path, 'category_label':result})


with open('result_a.csv','w',newline='') as csvFile:
  writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'attribute_label'])
  writer.writeheader()

  for result in outputa:
    file_path = dataset_test.file(idex)
    new = {'file_path':file_path}
    new['attribute_label'] = ''
    for i in range(len(result)):
      if(result[i] == 1):
        new['attribute_label'] += str(i)+" "
    idex+=1
    writer.writerow(new)
